{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donald/anaconda3/envs/tf36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Grid dt Parameters ...\n",
      "Fitting 5 folds for each of 6144 candidates, totalling 30720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 2196 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 13896 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 30720 out of 30720 | elapsed:   27.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-- Test MSE:0.017473682750651687\n",
      "{'criterion': 'mse', 'max_depth': 15, 'min_impurity_decrease': 1e-07, 'min_samples_leaf': 4, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.0, 'splitter': 'random'}\n",
      "\t-- Validation MSE of Model--best: 0.03270600649950598\n",
      "== Fill in submission ...\n",
      "== Process Successed!\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from utils import *\n",
    "from model import HousePriceModel\n",
    "\n",
    "\n",
    "\"\"\"House Price Predicting\n",
    "\n",
    "This Code Implement Ensemble Learning Capability, which means it can train on several (different) models, \n",
    "and integrate all the outputs as final outpus.\n",
    "\n",
    "Arguments are names of models, like xgb, nn. Configurations for models are set Default.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_data = pd.read_csv(PREPROCESSED_TRAINING_DATA_PATH)\n",
    "    train_data = train_data.drop(['Unnamed: 0'], axis=1)\n",
    "    train_data[\"SalePrice\"] = np.log1p(train_data[\"SalePrice\"])\n",
    "    test_data = pd.read_csv(PREPROCESSED_TEST_DATA_PATH)\n",
    "    test_data = test_data.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "    args = sys.argv[1:]\n",
    "\n",
    "    CLFs = HousePriceModel()\n",
    "\n",
    "    ensemble_models = []\n",
    "    \"\"\"\n",
    "    ensemble_models contains a set of models, to perform ensemble learning on several models.\n",
    "\n",
    "    It contains a List of models, for each #element:\n",
    "        [0. representation name,\n",
    "        1. model name,\n",
    "        2. [model config(dict), training config (dict)]\n",
    "\n",
    "        Sample cofig: (No specification is allowed, which means using default values)\n",
    "        \n",
    "            !!! Check those PARAMETERS on Sklearn Package !!!\n",
    "        \n",
    "            I. model config\n",
    "                a) xgb\n",
    "                    {\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'min_child_weight': min_child_weight,\n",
    "                    'booster': booster\n",
    "                    }\n",
    "                b) nn\n",
    "                    {\n",
    "                    'hidden_layer_sizes': hidden_layer_sizes,\n",
    "                    'activation': activation,\n",
    "                    'alpha': alpha,\n",
    "                    'learning_rate': learning_rate\n",
    "                    }\n",
    "\n",
    "            II. training config (dict)\n",
    "                a) xgb\n",
    "                    {\n",
    "                    'num_features': num_features,\n",
    "                    'split': split,\n",
    "                    'eval_set': eval_set,\n",
    "                    'eval_metric': eval_metric,\n",
    "                    'verbose': verbose,\n",
    "                    'xgb_model': xgb_model\n",
    "                    }\n",
    "                b) nn\n",
    "                    {\n",
    "                    'num_features': num_features,\n",
    "                    'split': split,\n",
    "                    }\n",
    "    \"\"\"\n",
    "#     if len(args) > 0:\n",
    "#         for arg in args:\n",
    "#             ensemble_models.append([arg, {}, {}])\n",
    "#     else:\n",
    "    grid_params = {\n",
    "            'xgb':{'learning_rate':[0.2, 0.1, 0.05, 0.02],\n",
    "                   'n_estimators':[100, 200, 300, 400, 500],\n",
    "                   'min_child_weight':[3, 4, 5, 6],\n",
    "                   'booster':['gbtree', 'gblinear', 'dart']},\n",
    "            'nn':{'hidden_layer_sizes':[(16,16)],\n",
    "                  'activation':['identity', 'relu','logistic'],\n",
    "                  'alpha':[0.0001, 0.0005, 0.001],\n",
    "                  'learning_rate':['adaptive', 'invscaling'],\n",
    "                  'max_iter':[200, 500, 1000]},\n",
    "            'svr':{'degree':[2, 3, 4],\n",
    "                   'kernel': ['rbf', 'poly', 'linear'],\n",
    "                   'gamma': [1e-3, 1e-4, 1e-5],\n",
    "                   'C': [1.0, 0.5, 2],\n",
    "                   'coef0':[0.0, 0.1, 0.2],\n",
    "                   'tol':[1e-3, 1e-4, 5e-4, 2e-3]},\n",
    "            'randF':{'n_estimators':[5, 10, 15, 20],\n",
    "                     'criterion':['mse'],\n",
    "                     'max_depth':[None, 10, 15, 20],\n",
    "                     'min_samples_split':[2, 5, 8, 10],\n",
    "                     'min_samples_leaf':[1, 3, 5]},\n",
    "            'bagging':{'base_estimator':[None],\n",
    "                       'n_estimators':[3, 7, 10, 13],\n",
    "                       'max_samples':[0.5, 0.8, 1.0],\n",
    "                       'max_features':[0.5, 0.8, 1.0],\n",
    "                       'bootstrap':[True, False]},\n",
    "            'logistic':{'penalty':['l2'],\n",
    "                        'dual':[False],\n",
    "                        'tol':[1e-4, 1e-5, 5e-5, 5e-4, 1e-3],\n",
    "                        'C':[0.2, 0.5, 1.0, 1.5]},\n",
    "            'dt':{'criterion':['mse'],\n",
    "                  'splitter':['best', 'random'],\n",
    "                  'max_depth':[None, 5, 8, 10, 12, 15],\n",
    "                  'min_samples_split':[2, 5, 10, 15],\n",
    "                  'min_samples_leaf':[1, 2, 4, 8],\n",
    "                  'min_weight_fraction_leaf':[0., 0.01, 0.02, 0.05],\n",
    "                 'min_impurity_decrease':[1e-7, 1e-6, 1e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]}\n",
    "        }\n",
    "\n",
    "    m = train_data.shape[0]\n",
    "    y_train = np.reshape(train_data['SalePrice'], newshape=(m,))\n",
    "    train_output_list = []\n",
    "    test_output_list = []\n",
    "\n",
    "    reps_list = ['dt']\n",
    "    for t in reps_list:\n",
    "        print(\"== Grid {} Parameters ...\".format(t))\n",
    "        # Create Dummy estimator\n",
    "        dummy_model = CLFs.add_model(t, name='dummy', config=[{}, {'split': 0}])\n",
    "        X_, y_ = CLFs.get_Xy(train_data, representation_name=t, name=dummy_model)\n",
    "        # GridSearch\n",
    "        best_model, best_params = grid(CLFs.models[t][dummy_model]['model'], grid_params[t], X_, y_)\n",
    "        print(best_params)\n",
    "        np.save('{}_cv_results.npy'.format(t), best_model.cv_results_)\n",
    "        # Add Best Model\n",
    "        best_xgb = CLFs.add_model(t,name='best',config=[best_params, {}])\n",
    "        # Fit Best Model\n",
    "        CLFs.fit(t, dataFrame=train_data, name=best_xgb)\n",
    "        # Evaluate on test set\n",
    "        X_test = CLFs.get_Xy(test_data, representation_name=t, name='best', bool_train=False)\n",
    "        test_output = CLFs.predict(representation_name=t, X=X_test, name='best')\n",
    "        test_output_list.append(test_output)\n",
    "        \n",
    "    final_output = np.exp(CLFs.ensemble_outputs(test_output_list))\n",
    "\n",
    "    print('== Fill in submission ...')\n",
    "    # Fill submission.csv\n",
    "    submission = pd.read_csv(SUBMISSION_PATH)\n",
    "    CLFs.fill_submission(final_output, submission)\n",
    "    # Delete dummy columns\n",
    "    sub_cols_all = submission.columns\n",
    "    sub_cols = ['Id', 'SalePrice']\n",
    "    for c in sub_cols_all:\n",
    "        if c not in sub_cols:\n",
    "            submission = submission.drop(columns=c)\n",
    "    # Save submission file\n",
    "    submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "    print(\"== Process Successed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('dt_output.npy', final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_criterion', 'param_max_depth', 'param_min_samples_leaf', 'param_min_samples_split', 'param_min_weight_fraction_leaf', 'param_splitter', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score', 'split0_train_score', 'split1_train_score', 'split2_train_score', 'split3_train_score', 'split4_train_score', 'mean_train_score', 'std_train_score'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_xgb.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse'\n",
      " 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse' 'mse']\n",
      "['best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random'\n",
      " 'best' 'random' 'best' 'random' 'best' 'random' 'best' 'random' 'best'\n",
      " 'random' 'best' 'random']\n",
      "[None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n",
      " 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n",
      " 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n",
      " 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n",
      " 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n",
      " 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15]\n",
      "[2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10 15 15 15 15 15 15\n",
      " 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10 15 15 15 15\n",
      " 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10 15 15\n",
      " 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10\n",
      " 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10\n",
      " 10 10 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10\n",
      " 10 10 10 10 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10\n",
      " 10 10 10 10 10 10 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5\n",
      " 10 10 10 10 10 10 10 10 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5\n",
      " 5 5 5 10 10 10 10 10 10 10 10 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5\n",
      " 5 5 5 5 5 5 10 10 10 10 10 10 10 10 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2\n",
      " 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10 15 15 15 15 15 15 15 15 2 2 2 2\n",
      " 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10 15 15 15 15 15 15 15 15 2\n",
      " 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10 15 15 15 15 15 15\n",
      " 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10 15 15 15 15\n",
      " 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10 15 15\n",
      " 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10\n",
      " 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10\n",
      " 10 10 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10\n",
      " 10 10 10 10 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10\n",
      " 10 10 10 10 10 10 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5\n",
      " 10 10 10 10 10 10 10 10 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5 5 5 5\n",
      " 5 5 5 10 10 10 10 10 10 10 10 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2 2 5 5\n",
      " 5 5 5 5 5 5 10 10 10 10 10 10 10 10 15 15 15 15 15 15 15 15 2 2 2 2 2 2 2\n",
      " 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10 15 15 15 15 15 15 15 15 2 2 2 2\n",
      " 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10 15 15 15 15 15 15 15 15]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05\n",
      " 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02\n",
      " 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02\n",
      " 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01\n",
      " 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01\n",
      " 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0\n",
      " 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0\n",
      " 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05\n",
      " 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05\n",
      " 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02\n",
      " 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02\n",
      " 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01\n",
      " 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01\n",
      " 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0\n",
      " 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0\n",
      " 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05\n",
      " 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05\n",
      " 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02\n",
      " 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02\n",
      " 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01\n",
      " 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01\n",
      " 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0\n",
      " 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0\n",
      " 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05\n",
      " 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05\n",
      " 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02\n",
      " 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02\n",
      " 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01\n",
      " 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01\n",
      " 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0\n",
      " 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0\n",
      " 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05\n",
      " 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05\n",
      " 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02\n",
      " 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02\n",
      " 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01\n",
      " 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01\n",
      " 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0\n",
      " 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0\n",
      " 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05\n",
      " 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05\n",
      " 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02\n",
      " 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02\n",
      " 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01\n",
      " 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01\n",
      " 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0\n",
      " 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0\n",
      " 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05\n",
      " 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05\n",
      " 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02\n",
      " 0.05 0.05 0.0 0.0 0.01 0.01 0.02 0.02 0.05 0.05 0.0 0.0 0.01 0.01 0.02\n",
      " 0.02 0.05 0.05]\n"
     ]
    }
   ],
   "source": [
    "for k in  grid_params['dt'].keys():\n",
    "    try:\n",
    "        print(best_model_xgb.cv_results_['param_{}'.format(k)])\n",
    "    except:\n",
    "        print('No such param {}'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
